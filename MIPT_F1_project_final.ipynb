{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7f8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# ВЫГРУЗКА СТАТЕЙ ИЗ ВИКИПЕДИИ О ФОРМУЛЕ 1 НА РУССКОМ ЯЗЫКЕ\n",
    "\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import wikipediaapi\n",
    "import mwparserfromhell\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.parse import unquote\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import random\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "\n",
    "def safe_filename(name):\n",
    "    # Удаляем/заменяем символы, недопустимые в именах файлов\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', '_', name)\n",
    "\n",
    "class F1WikiRawSaver:\n",
    "    def __init__(self, lang='ru', storage_path='D:/f1_f1wiki_raw', max_workers=4):\n",
    "        self.wiki = wikipediaapi.Wikipedia(\n",
    "            language=lang,\n",
    "            user_agent=\"F1WikiRawSaver/1.1 (research@example.com)\",\n",
    "            extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    "        )\n",
    "        self.storage_path = os.path.abspath(storage_path)\n",
    "        self.max_workers = max_workers\n",
    "        self.visited_articles = set() \n",
    "        self.visited_categories = set()\n",
    "        self.failed_articles = []\n",
    "        self.categories = [\n",
    "            \"Категория:Гран-при Формулы-1\",\n",
    "            \"Категория:Этапы Формулы-1\",\n",
    "            \"Категория:Сезоны Формулы-1\",\n",
    "            \"Категория:Чемпионаты Формулы-1\",\n",
    "            \"Категория:Конструкторы Формулы-1\",\n",
    "            \"Категория:Команды Формулы-1\",\n",
    "            \"Категория:Гонщики Формулы-1\",\n",
    "            \"Категория:Пилоты Формулы-1\",\n",
    "            \"Категория:Двигатели Формулы-1\",\n",
    "            \"Категория:Моторы Формулы-1\",\n",
    "            \"Категория:Шины Формулы-1\",\n",
    "            \"Категория:Покрышки Формулы-1\",\n",
    "            \"Категория:Автодромы Формулы-1\",\n",
    "            \"Категория:Трассы Формулы-1\",\n",
    "            \"Категория:Аварии в Формуле-1\",\n",
    "            \"Категория:Происшествия в Формуле-1\",\n",
    "            \"Категория:История Формулы-1\",\n",
    "            \"Категория:Инженеры Формулы-1\",\n",
    "            \"Категория:Техники Формулы-1\",\n",
    "            \"Категория:Руководители Формулы-1\",\n",
    "            \"Категория:Владельцы команд Формулы-1\"\n",
    "        ]\n",
    "        self._setup_directory_structure()\n",
    "\n",
    "    def _setup_directory_structure(self):\n",
    "        os.makedirs(os.path.join(self.storage_path, 'raw'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.storage_path, 'logs'), exist_ok=True)\n",
    "\n",
    "    def _get_all_titles(self, category, depth=2):\n",
    "        titles = set()\n",
    "        try:\n",
    "            category_page = self.wiki.page(category)\n",
    "            if not category_page.exists():\n",
    "                alt_category = category.replace(\"Формулы-1\", \"Формулы 1\")\n",
    "                category_page = self.wiki.page(alt_category)\n",
    "                if not category_page.exists():\n",
    "                    logging.warning(f\"Категория не найдена: {category}\")\n",
    "                    return titles\n",
    "            for title, member in category_page.categorymembers.items():\n",
    "                if member.ns == wikipediaapi.Namespace.CATEGORY and depth > 0:\n",
    "                    if title in self.visited_categories:\n",
    "                        continue\n",
    "                    self.visited_categories.add(title)\n",
    "                    titles.update(self._get_all_titles(title, depth-1))\n",
    "                elif member.ns == wikipediaapi.Namespace.MAIN:\n",
    "                    if (title, category) in self.visited_articles:\n",
    "                        continue\n",
    "                    self.visited_articles.add((title, category))\n",
    "                    titles.add((title, category))\n",
    "                time.sleep(0.1)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Ошибка при обработке категории {category}: {str(e)}\")\n",
    "        return titles\n",
    "\n",
    "    def _save_article(self, title, text):\n",
    "        filename = safe_filename(title) + \".txt\"\n",
    "        filepath = os.path.join(self.storage_path, 'raw', filename)\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "\n",
    "    def _process_article(self, title, category):\n",
    "        base_delay = 1.0  # начальная задержка между попытками\n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                page = self.wiki.page(title)\n",
    "                if not page.exists() or not page.text.strip():\n",
    "                    self.failed_articles.append({\n",
    "                        'title': title,\n",
    "                        'category': category,\n",
    "                        'reason': 'not_found_or_empty',\n",
    "                        'timestamp': datetime.now().isoformat()\n",
    "                    })\n",
    "                    return\n",
    "                # Очищаем текст, сохраняем даже незаконченные/короткие\n",
    "                try:\n",
    "                    parsed = mwparserfromhell.parse(page.text)\n",
    "                    clean_text = parsed.strip_code()\n",
    "                except Exception:\n",
    "                    clean_text = page.text\n",
    "                self._save_article(title, clean_text)\n",
    "                return\n",
    "            except Exception as e:\n",
    "                delay = base_delay * (2 ** attempt) + random.uniform(0, 0.5)\n",
    "                logging.warning(f\"Попытка {attempt+1} для '{title}' ({category}): {str(e)}. Жду {delay:.1f} сек.\")\n",
    "                time.sleep(delay)\n",
    "        self.failed_articles.append({\n",
    "            'title': title,\n",
    "            'category': category,\n",
    "            'reason': 'max_attempts_failed',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "    def collect_and_save_all(self):\n",
    "        all_titles = set()\n",
    "        for category in self.categories:\n",
    "            logging.info(f\"Сбор: {category}\")\n",
    "            all_titles.update(self._get_all_titles(category))\n",
    "        logging.info(f\"Всего уникальных пар (статья, категория): {len(all_titles)}\")\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = []\n",
    "            for title, category in all_titles:\n",
    "                futures.append(executor.submit(self._process_article, title, category))\n",
    "            for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Сохранение статей\"):\n",
    "                pass\n",
    "        # Сохраняем логи\n",
    "        if self.failed_articles:\n",
    "            failed_path = os.path.join(self.storage_path, 'logs', 'failed_articles.json')\n",
    "            with open(failed_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.failed_articles, f, ensure_ascii=False, indent=2)\n",
    "            logging.info(f\"Ошибок при обработке статей: {len(self.failed_articles)} (см. {failed_path})\")\n",
    "        # Сохраняем прогресс\n",
    "        progress_path = os.path.join(self.storage_path, 'logs', 'progress.json')\n",
    "        with open(progress_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'visited_articles': [list(x) for x in self.visited_articles],\n",
    "                'visited_categories': list(self.visited_categories),\n",
    "                'total': len(self.visited_articles),\n",
    "                'last_saved': datetime.now().isoformat()\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    logging.info(\"Запуск сохранения всех статей Формулы 1 из Википедии\")\n",
    "    saver = F1WikiRawSaver(\n",
    "        storage_path='D:/f1_f1wiki_raw',\n",
    "        max_workers=4\n",
    "    )\n",
    "    saver.collect_and_save_all()\n",
    "    print(f\"\\n⏱️ Общее время выполнения: {(time.time()-start_time)/60:.1f} минут\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# ФОРМИРОВАНИЕ КОРПУСА ТЕКСТОВ О ФОРМУЛЕ 1 НА ОСНОВЕ СТАТЕЙ ИЗ ВИКИПЕДИИ\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "RAW_DIR = 'D:/f1_f1wiki_raw/raw'  # путь к папке\n",
    "OUT_CSV = 'D:/f1_f1wiki_raw/processed/f1_corpus.csv'\n",
    "OUT_JSONL = 'D:/f1_f1wiki_raw/processed/f1_corpus.jsonl'\n",
    "OUT_TXT = 'D:/f1_f1wiki_raw/processed/f1_corpus.txt'\n",
    "\n",
    "os.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\n",
    "\n",
    "def get_file_metadata(filepath):\n",
    "    stat = os.stat(filepath)\n",
    "    return {\n",
    "        'created': datetime.fromtimestamp(stat.st_ctime).isoformat(),\n",
    "        'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()\n",
    "    }\n",
    "\n",
    "def build_corpus(raw_dir):\n",
    "    data = []\n",
    "    for fname in tqdm(os.listdir(raw_dir), desc=\"Чтение файлов\"):\n",
    "        if not fname.endswith('.txt'):\n",
    "            continue\n",
    "        path = os.path.join(raw_dir, fname)\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "            title = fname[:-4]\n",
    "            meta = get_file_metadata(path)\n",
    "            data.append({\n",
    "                'title': title,\n",
    "                'text': text,\n",
    "                'length': len(text),\n",
    "                'file': fname,\n",
    "                'created': meta['created'],\n",
    "                'modified': meta['modified']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при чтении {fname}: {e}\")\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = build_corpus(RAW_DIR)\n",
    "    print(f\"Всего статей: {len(corpus)}\")\n",
    "    # Сохраняем в CSV\n",
    "    df = pd.DataFrame(corpus)\n",
    "    df.to_csv(OUT_CSV, index=False, encoding='utf-8')\n",
    "    print(f\"Сохранено в {OUT_CSV}\")\n",
    "    # Сохраняем в JSONL (для LLM)\n",
    "    with open(OUT_JSONL, 'w', encoding='utf-8') as f:\n",
    "        for row in corpus:\n",
    "            f.write(json.dumps(row, ensure_ascii=False) + '\\n')\n",
    "    print(f\"Сохранено в {OUT_JSONL}\")\n",
    "    # Сохраняем в TXT\n",
    "    with open(OUT_TXT, 'w', encoding='utf-8') as f:\n",
    "        for row in corpus:\n",
    "            f.write(f\"### {row['title']}\\n{row['text']}\\n\\n\")\n",
    "    print(f\"Сохранено в {OUT_TXT}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# ПОДГОТОВКА И РАЗМЕТКА ДАТАСЕТА (ФОРМИРОВАНИЕ ПАР ВОПРОС-ОТВЕТ) НА ОСНОВЕ КОРПУСА ТЕКСТОВ\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# ========== 1. ПУТИ ==========\n",
    "input_path = r\"D:\\f1_f1wiki_raw\\processed\\f1_corpus.jsonl\"  # Исходный корпус\n",
    "output_dir = r\"D:\\f1_f1wiki_raw\\processed\"\n",
    "dataset_path = os.path.join(output_dir, \"f1_qa_dataset.jsonl\")  # датасет\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ========== 2. ФУНКЦИИ ==========\n",
    "\n",
    "# Функция очистки текста\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Функция генерации текста\n",
    "def generate(text, **kwargs):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        hypotheses = model.generate(**inputs, num_beams=2, max_length=64, **kwargs)\n",
    "    return tokenizer.decode(hypotheses[0], skip_special_tokens=True)\n",
    "\n",
    "# ========== 3. ЗАГРУЗКА МОДЕЛИ ==========\n",
    "model_name = \"cointegrated/rut5-base-multitask\"  # Основная модель\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Загрузка модели...\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Модель загружена!\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка загрузки модели: {e}\")\n",
    "    raise\n",
    "\n",
    "# ========== 4. ТЕСТОВАЯ ОБРАБОТКА ==========\n",
    "try:\n",
    "    print(\"Начало тестовой обработки на 50 записях...\")\n",
    "    count = 0\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as infile, open(test_dataset_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in tqdm(infile, desc=\"Тестирование\", total=2727):\n",
    "            if count >= 2727:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                context = clean_text(data[\"text\"])\n",
    "                \n",
    "                # Пропуск коротких текстов\n",
    "                if len(context.split()) < 50:\n",
    "                    continue\n",
    "                \n",
    "                # Генерация вопроса\n",
    "                question = generate(f\"ask | {context[:1024]}\")\n",
    "                if not question or question.lower() in context.lower():\n",
    "                    continue  # Пропуск пустых/повторяющихся вопросов\n",
    "                \n",
    "                # Извлечение ответа\n",
    "                answer = generate(f\"comprehend | {context[:1024]}. Вопрос: {question}?\")\n",
    "                if not answer or answer.lower() not in context.lower():\n",
    "                    continue  # Пропуск некорректных ответов\n",
    "                \n",
    "                # Сохранение пары\n",
    "                sample = {\n",
    "                    \"title\": data[\"title\"],\n",
    "                    \"context\": context,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer\n",
    "                }\n",
    "                outfile.write(json.dumps(sample, ensure_ascii=False) + \"\\n\")\n",
    "                count += 1\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                continue  # Пропуск поврежденных строк\n",
    "    \n",
    "    print(f\"Тестовый датасет создан: {dataset_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка тестовой обработки: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# QA МОДЕЛЬ, ОБУЧЕНИЕ, ПОЛЬЗОВАТЕЛЬСКИЙ ИНТЕРФЕЙС, МОДУЛЬ ДООБУЧЕНИЯ МОДЕЛИ\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import Dataset, DatasetDict\n",
    "from evaluate import load\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "\n",
    "# ========== 1. ПУТИ ==========\n",
    "input_path = r\"D:\\f1_f1wiki_raw\\processed\\f1_qa_dataset.jsonl\"\n",
    "output_dir = r\"D:\\f1_f1wiki_raw\\processed\"\n",
    "model_output_dir = os.path.join(output_dir, \"rubert_tiny2_qa\")\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "# ========== 2. ФУНКЦИИ ==========\n",
    "\n",
    "def normalize_text(text):\n",
    "    return re.sub(r'\\s+', ' ', re.sub(r'[^а-яА-Яa-zA-Z0-9\\s]', '', text)).strip()\n",
    "\n",
    "def is_answer_in_context(context, answer):\n",
    "    return normalize_text(answer) in normalize_text(context)\n",
    "\n",
    "def remove_duplicates(data):\n",
    "    seen = set()\n",
    "    unique_data = []\n",
    "    for item in data:\n",
    "        key = (normalize_text(item[\"question\"]), normalize_text(item[\"context\"]))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_data.append(item)\n",
    "    print(f\"Удалено дубликатов: {len(data) - len(unique_data)}\")\n",
    "    return unique_data\n",
    "\n",
    "def prepare_train_features(examples, tokenizer, max_length=384, doc_stride=128):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sample_idx = sample_mapping[i]\n",
    "        answer = examples[\"answer\"][sample_idx]\n",
    "        context = examples[\"context\"][sample_idx]\n",
    "        answer_start = context.find(answer)\n",
    "        if answer_start == -1:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            continue\n",
    "        answer_end = answer_start + len(answer) - 1\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != context_index:\n",
    "            token_start_index += 1\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != context_index:\n",
    "            token_end_index -= 1\n",
    "        while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start:\n",
    "            token_start_index += 1\n",
    "        tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "        while offsets[token_end_index][1] >= answer_end:\n",
    "            token_end_index -= 1\n",
    "        tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "    return tokenized_examples\n",
    "\n",
    "def compute_metrics(p, dataset, tokenizer):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for i in range(len(p.predictions)):\n",
    "        context = dataset[i][\"context\"]\n",
    "        question = dataset[i][\"question\"]\n",
    "        true_answer = dataset[i][\"answer\"]\n",
    "        inputs = tokenizer(question, context, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "        start_index = torch.argmax(start_logits)\n",
    "        end_index = torch.argmax(end_logits)\n",
    "        pred_answer = tokenizer.decode(inputs[\"input_ids\"][0][start_index:end_index+1], skip_special_tokens=True)\n",
    "        predictions.append({\"prediction_text\": pred_answer, \"id\": str(i)})\n",
    "        references.append({\n",
    "            \"id\": str(i),\n",
    "            \"answers\": {\n",
    "                \"text\": [true_answer],\n",
    "                \"answer_start\": [context.find(true_answer)]\n",
    "            }\n",
    "        })\n",
    "    return metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "def plot_comparison(train_metrics, test_metrics):\n",
    "    metrics = ['exact_match', 'f1']\n",
    "    plt.figure(figsize=(10,5))\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    plt.bar(x - width/2, [train_metrics[m] for m in metrics], width, label='Train', color='skyblue')\n",
    "    plt.bar(x + width/2, [test_metrics[m] for m in metrics], width, label='Test', color='salmon')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 100)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(model_output_dir, 'performance_comparison.png'))\n",
    "    plt.show()\n",
    "\n",
    "# ========== НОВЫЕ ФУНКЦИИ ==========\n",
    "\n",
    "# Инициализация модели эмбеддингов\n",
    "embedder = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "def create_context_embeddings(dataset):\n",
    "    contexts = [normalize_text(item[\"context\"]) for item in dataset]\n",
    "    embeddings = embedder.encode(contexts, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "def create_faiss_index(context_embeddings):\n",
    "    dimension = context_embeddings.shape[1]\n",
    "    nlist = 100\n",
    "    quantizer = faiss.IndexFlatL2(dimension)\n",
    "    index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)\n",
    "    index.train(context_embeddings.numpy())\n",
    "    index.add(context_embeddings.numpy())\n",
    "    return index\n",
    "\n",
    "def filter_context_by_keywords(question, context):\n",
    "    question_words = set(normalize_text(question).split())\n",
    "    context_words = set(normalize_text(context).split())\n",
    "    return len(question_words & context_words) > 0\n",
    "\n",
    "# ========== ОСНОВНОЙ КОД ==========\n",
    "\n",
    "# ========== 3. ЗАГРУЗКА МОДЕЛИ И ТОКЕНИЗАТОРА ==========\n",
    "model_name = \"cointegrated/rubert-tiny2\"\n",
    "try:\n",
    "    print(\"Загрузка модели и токенизатора...\")\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "    model = BertForQuestionAnswering.from_pretrained(model_name).to(\"cpu\")\n",
    "    print(\"Модель успешно загружена!\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка загрузки модели: {e}\")\n",
    "    raise\n",
    "\n",
    "# ========== 4. ЗАГРУЗКА И ПРЕДОБРАБОТКА ДАННЫХ ==========\n",
    "try:\n",
    "    print(\"Загрузка датасета...\")\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = [json.loads(line) for line in f if line.strip()]\n",
    "    filtered_data = [\n",
    "        {\n",
    "            \"context\": item[\"context\"],\n",
    "            \"question\": item[\"question\"],\n",
    "            \"answer\": item[\"answer\"]\n",
    "        } for item in raw_data if is_answer_in_context(item[\"context\"], item[\"answer\"])\n",
    "    ]\n",
    "    unique_data = remove_duplicates(filtered_data)\n",
    "    dataset = Dataset.from_list(unique_data).train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "    print(f\"Датасет загружен: {len(dataset['train'])} тренировочных записей, {len(dataset['test'])} тестовых\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка загрузки датасета: {e}\")\n",
    "    raise\n",
    "\n",
    "# ========== 5. ТОКЕНИЗАЦИЯ ==========\n",
    "try:\n",
    "    print(\"Токенизация данных...\")\n",
    "    tokenized_datasets = dataset.map(\n",
    "        lambda x: prepare_train_features(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=[\"context\", \"question\", \"answer\"]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка токенизации: {e}\")\n",
    "    raise\n",
    "\n",
    "# ========== 6. ОЦЕНКА КАЧЕСТВА ==========\n",
    "try:\n",
    "    print(\"Загрузка метрики SQuAD...\")\n",
    "    metric = load(\"squad\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка загрузки метрики: {e}\")\n",
    "    raise\n",
    "\n",
    "# ========== 7. ОБУЧЕНИЕ МОДЕЛИ ==========\n",
    "try:\n",
    "    print(\"Настройка параметров обучения...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=10,\n",
    "        learning_rate=1e-5,\n",
    "        weight_decay=0.1,\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, dataset[\"test\"], tokenizer),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    print(\"Начало обучения...\")\n",
    "    trainer.train()\n",
    "    print(\"Обучение завершено!\")\n",
    "    trainer.save_model(model_output_dir)\n",
    "    print(f\"Модель сохранена: {model_output_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка обучения: {e}\")\n",
    "    raise\n",
    "\n",
    "# ========== 8. СРАВНЕНИЕ КАЧЕСТВА TRAIN/TEST ==========\n",
    "try:\n",
    "    print(\"Сравнение качества на train и test...\")\n",
    "    train_metrics = compute_metrics(trainer.predict(tokenized_datasets[\"train\"]), dataset[\"train\"], tokenizer)\n",
    "    test_metrics = compute_metrics(trainer.predict(tokenized_datasets[\"test\"]), dataset[\"test\"], tokenizer)\n",
    "    print(\"Метрики на train:\", {k: f\"{v:.2f}\" for k, v in train_metrics.items()})\n",
    "    print(\"Метрики на test:\", {k: f\"{v:.2f}\" for k, v in test_metrics.items()})\n",
    "    plot_comparison(train_metrics, test_metrics)\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка сравнения качества: {e}\")\n",
    "\n",
    "# ========== 9. ПОЛЬЗОВАТЕЛЬСКИЙ ИНТЕРФЕЙС ==========\n",
    "# Создание эмбеддингов и FAISS-индекса\n",
    "context_embeddings = create_context_embeddings(dataset[\"train\"])\n",
    "faiss_index = create_faiss_index(context_embeddings)\n",
    "\n",
    "def predict_answer(question):\n",
    "    question = normalize_text(question)\n",
    "    question_embedding = embedder.encode([question])\n",
    "    distances, indices = faiss_index.search(question_embedding, 10)\n",
    "    for idx in indices[0]:\n",
    "        index = int(idx)\n",
    "        if 0 <= index < len(dataset[\"train\"]):\n",
    "            context = dataset[\"train\"][index][\"context\"]\n",
    "            if filter_context_by_keywords(question, context):\n",
    "                break\n",
    "    else:\n",
    "        context = \"Контекст не найден\"\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        start_index = torch.argmax(outputs.start_logits)\n",
    "        end_index = torch.argmax(outputs.end_logits)\n",
    "        answer = tokenizer.decode(inputs[\"input_ids\"][0][start_index:end_index+1], skip_special_tokens=True)\n",
    "    return answer, context\n",
    "\n",
    "# ========== 10. ПОЛЬЗОВАТЕЛЬСКИЙ ИНТЕРФЕЙС С ОЦЕНКОЙ ==========\n",
    "def collect_feedback():\n",
    "    feedback_data = []\n",
    "    print(\"Задайте вопросы для оценки (введите 'exit' для выхода):\")\n",
    "    while True:\n",
    "        question = input(\"Вы: \").strip()\n",
    "        if question.lower() == \"exit\":\n",
    "            break\n",
    "        answer, context = predict_answer(question)\n",
    "        print(f\"Ответ: {answer}\")  # Только ответ\n",
    "        try:\n",
    "            rating = int(input(\"Оцените ответ (1-5): \"))  # Оценка после ответа\n",
    "            if 1 <= rating <= 5:\n",
    "                feedback_data.append({\n",
    "                    \"question\": question,\n",
    "                    \"context\": context,\n",
    "                    \"answer\": answer,\n",
    "                    \"user_rating\": rating\n",
    "                })\n",
    "            else:\n",
    "                print(\"Неверная оценка. Пропущено.\")\n",
    "        except ValueError:\n",
    "            print(\"Пожалуйста, введите число от 1 до 5.\")\n",
    "    return feedback_data\n",
    "\n",
    "# ========== 11. ДООБУЧЕНИЕ ==========\n",
    "def retrain_model(feedback_data):\n",
    "    if not feedback_data:\n",
    "        print(\"Нет данных для дообучения\")\n",
    "        return\n",
    "    print(f\"Добавлено {len(feedback_data)} примеров для дообучения\")\n",
    "    low_rated = [item for item in feedback_data if item[\"user_rating\"] <= 3]\n",
    "    if not low_rated:\n",
    "        print(\"Нет примеров с низкой оценкой. Дообучение не требуется\")\n",
    "        return\n",
    "    new_dataset = Dataset.from_list([\n",
    "        {\"context\": d[\"context\"], \"question\": d[\"question\"], \"answer\": d[\"answer\"]}\n",
    "        for d in low_rated\n",
    "    ]).train_test_split(test_size=0.1, seed=42)\n",
    "    global dataset\n",
    "    dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "    dataset[\"train\"] = dataset[\"train\"].concatenate(new_dataset[\"train\"])\n",
    "    global context_embeddings, faiss_index\n",
    "    context_embeddings = create_context_embeddings(dataset[\"train\"])\n",
    "    faiss_index = create_faiss_index(context_embeddings)\n",
    "    global tokenized_datasets\n",
    "    tokenized_datasets = dataset.map(\n",
    "        lambda x: prepare_train_features(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=[\"context\", \"question\", \"answer\"]\n",
    "    )\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=1e-5,\n",
    "        weight_decay=0.1,\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, dataset[\"test\"], tokenizer),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "    print(\"Начало дообучения...\")\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_output_dir)\n",
    "    print(\"Модель обновлена!\")\n",
    "\n",
    "# ========== 12. ОСНОВНОЙ ЦИКЛ ==========\n",
    "if __name__ == \"__main__\":\n",
    "    # Интерфейс с оценкой\n",
    "    feedback_data = collect_feedback()\n",
    "    # Дообучение на основе оценок\n",
    "    if feedback_data:\n",
    "        retrain_model(feedback_data)\n",
    "    print(\"Модель готова к использованию!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
